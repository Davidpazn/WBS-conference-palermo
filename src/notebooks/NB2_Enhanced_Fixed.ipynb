{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed Collection Setup and RAG Search\n",
    "\n",
    "This notebook contains the corrected Qdrant collection setup and search functionality that works with the backend tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Added to sys.path: /home/david/WBS-conference-palermo/src\n",
      "✅ Loaded environment from /home/david/WBS-conference-palermo/src/infra/.env\n",
      "✅ Enhanced environment setup complete with backend integration\n",
      "✅ OTLP exporter configured for endpoint: http://localhost:4317\n",
      "✅ LangSmith client initialized\n",
      "✅ OpenTelemetry + LangSmith tracing configured\n",
      "✅ Backend modules imported successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 1s.\n",
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 2s.\n",
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 4s.\n",
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 8s.\n",
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 16s.\n",
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 32s.\n",
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 1s.\n",
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 2s.\n",
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 4s.\n",
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 8s.\n",
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 16s.\n",
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 32s.\n",
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 1s.\n",
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 2s.\n",
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 4s.\n",
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 8s.\n",
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 16s.\n",
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 32s.\n",
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 1s.\n",
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 2s.\n",
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 4s.\n",
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 8s.\n",
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 16s.\n",
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 32s.\n"
     ]
    }
   ],
   "source": [
    "# Fixed Qdrant Collection Setup with Backend Integration\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Add src to path for backend imports\n",
    "def add_src_to_path():\n",
    "    \"\"\"Add src directory to Python path for backend imports\"\"\"\n",
    "    here = Path.cwd().resolve()\n",
    "    for base in [here, *here.parents]:\n",
    "        src_dir = base / \"src\"\n",
    "        if src_dir.is_dir() and (src_dir / \"backend\").is_dir():\n",
    "            sys.path.insert(0, str(src_dir))\n",
    "            print(f\"✅ Added to sys.path: {src_dir}\")\n",
    "            return src_dir\n",
    "    raise FileNotFoundError(\"Could not locate 'src/' with 'backend/' package\")\n",
    "\n",
    "src_path = add_src_to_path()\n",
    "\n",
    "# Enhanced environment setup with backend integration\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Literal\n",
    "\n",
    "# Load environment variables early\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    # Load from multiple possible locations\n",
    "    env_files = [\n",
    "        Path.cwd() / \".env\",\n",
    "        Path.cwd().parent / \".env\", \n",
    "        Path.cwd().parent / \"infra/.env\"\n",
    "    ]\n",
    "    for env_file in env_files:\n",
    "        if env_file.exists():\n",
    "            load_dotenv(env_file, override=False)\n",
    "            print(f\"✅ Loaded environment from {env_file}\")\n",
    "            break\n",
    "except ImportError:\n",
    "    print(\"⚠️ python-dotenv not available, using system environment\")\n",
    "\n",
    "print(\"✅ Enhanced environment setup complete with backend integration\")\n",
    "\n",
    "# Import backend modules\n",
    "from backend.app.tools.qdrant_admin import (\n",
    "    ensure_collection_edgar, search_dense_by_text, build_filter\n",
    ")\n",
    "from backend.app.ingest.config import EMBED_MODEL, VECTOR_SIZE\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct\n",
    "from openai import OpenAI\n",
    "\n",
    "print(\"✅ Backend modules imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Clients initialized\n",
      "Qdrant URL: https://6f0ea8e1-af5c-4424-b7de-63068430c352.us-east4-0.gcp.cloud.qdrant.io:6333/\n",
      "Embed model: text-embedding-3-small\n",
      "Vector size: 1536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_540476/3318804940.py:3: UserWarning: Qdrant client version 1.15.1 is incompatible with server version 1.12.1. Major versions should match and minor version difference must not exceed 1. Set check_compatibility=False to skip version check.\n",
      "  qdrant_client = QdrantClient(\n"
     ]
    }
   ],
   "source": [
    "# Initialize clients\n",
    "openai_client = OpenAI()  # Uses OPENAI_API_KEY from environment\n",
    "qdrant_client = QdrantClient(\n",
    "    url=os.getenv('QDRANT_URL', 'http://localhost:6333'),\n",
    "    api_key=os.getenv('QDRANT_API_KEY'),\n",
    "    timeout=30\n",
    ")\n",
    "\n",
    "COLLECTION_NAME = \"nb2_fixed_portfolio_rules\"\n",
    "\n",
    "print(f\"✅ Clients initialized\")\n",
    "print(f\"Qdrant URL: {os.getenv('QDRANT_URL', 'http://localhost:6333')}\")\n",
    "print(f\"Embed model: {EMBED_MODEL}\")\n",
    "print(f\"Vector size: {VECTOR_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Collection 'nb2_fixed_portfolio_rules' created successfully\n",
      "Vector size: 1536\n",
      "Distance metric: Cosine\n"
     ]
    }
   ],
   "source": [
    "# Create collection with proper error handling\n",
    "try:\n",
    "    ensure_collection_edgar(\n",
    "        client=qdrant_client,\n",
    "        name=COLLECTION_NAME,\n",
    "        vector_size=VECTOR_SIZE,\n",
    "        recreate=True  # Start fresh\n",
    "    )\n",
    "    print(f\"✅ Collection '{COLLECTION_NAME}' created successfully\")\n",
    "    \n",
    "    # Get collection info\n",
    "    collection_info = qdrant_client.get_collection(COLLECTION_NAME)\n",
    "    print(f\"Vector size: {collection_info.config.params.vectors.size}\")\n",
    "    print(f\"Distance metric: {collection_info.config.params.vectors.distance}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Collection creation failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 6 compliance rules prepared\n"
     ]
    }
   ],
   "source": [
    "# Enhanced compliance rules for testing\n",
    "enhanced_compliance_rules = [\n",
    "    {\n",
    "        \"text\": \"No single equity position shall exceed 25% of total portfolio value to maintain diversification and limit concentration risk per Basel III guidelines.\",\n",
    "        \"category\": \"concentration_limits\",\n",
    "        \"severity\": \"high\",\n",
    "        \"regulation\": \"basel_iii\",\n",
    "        \"last_updated\": \"2025-01-01\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Technology sector allocation should not exceed 35% of total portfolio to avoid overexposure to sector-specific risks and market volatility.\",\n",
    "        \"category\": \"sector_limits\", \n",
    "        \"severity\": \"medium\",\n",
    "        \"regulation\": \"internal_policy\",\n",
    "        \"last_updated\": \"2025-01-01\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Minimum trade size must be $1,000 to ensure cost-effective execution and avoid excessive transaction costs that erode returns.\",\n",
    "        \"category\": \"trade_execution\",\n",
    "        \"severity\": \"low\",\n",
    "        \"regulation\": \"mifid_ii\",\n",
    "        \"last_updated\": \"2025-01-01\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Cash allocation should remain between 5-15% for liquidity management and opportunistic investments during market volatility.\",\n",
    "        \"category\": \"liquidity\",\n",
    "        \"severity\": \"medium\",\n",
    "        \"regulation\": \"internal_policy\",\n",
    "        \"last_updated\": \"2025-01-01\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"ESG score below 6.0 requires additional review and justification for inclusion in sustainable portfolios per EU Taxonomy requirements.\",\n",
    "        \"category\": \"esg_compliance\",\n",
    "        \"severity\": \"high\",\n",
    "        \"regulation\": \"eu_taxonomy\",\n",
    "        \"last_updated\": \"2025-01-01\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Cryptocurrency exposure must not exceed 5% of total portfolio value due to regulatory uncertainty and volatility risks.\",\n",
    "        \"category\": \"alternative_assets\",\n",
    "        \"severity\": \"high\",\n",
    "        \"regulation\": \"sec_guidance\",\n",
    "        \"last_updated\": \"2025-01-01\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"✅ {len(enhanced_compliance_rules)} compliance rules prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 6 rules...\n",
      "✅ Generated 6 embeddings\n",
      "Embedding dimension: 1536\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings and create points\n",
    "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "    \"\"\"Generate embeddings using OpenAI with proper error handling\"\"\"\n",
    "    try:\n",
    "        response = openai_client.embeddings.create(\n",
    "            model=EMBED_MODEL,\n",
    "            input=texts\n",
    "        )\n",
    "        return [item.embedding for item in response.data]\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Embedding generation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Embed rules\n",
    "rule_texts = [rule[\"text\"] for rule in enhanced_compliance_rules]\n",
    "print(f\"Generating embeddings for {len(rule_texts)} rules...\")\n",
    "\n",
    "embeddings = embed_texts(rule_texts)\n",
    "print(f\"✅ Generated {len(embeddings)} embeddings\")\n",
    "print(f\"Embedding dimension: {len(embeddings[0])}\")\n",
    "\n",
    "# Verify dimension matches collection\n",
    "if len(embeddings[0]) != VECTOR_SIZE:\n",
    "    raise ValueError(f\"Embedding dimension {len(embeddings[0])} doesn't match collection vector size {VECTOR_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 6 points for upsert\n",
      "✅ Successfully upserted 6 points\n",
      "Collection now contains 6 points\n"
     ]
    }
   ],
   "source": [
    "# Create and upsert points\n",
    "points = [\n",
    "    PointStruct(\n",
    "        id=i,\n",
    "        vector=embedding,\n",
    "        payload={\n",
    "            \"text\": rule[\"text\"],\n",
    "            \"category\": rule[\"category\"],\n",
    "            \"severity\": rule[\"severity\"],\n",
    "            \"regulation\": rule[\"regulation\"],\n",
    "            \"last_updated\": rule[\"last_updated\"],\n",
    "            \"source\": \"nb2_fixed_rules\"\n",
    "        }\n",
    "    )\n",
    "    for i, (rule, embedding) in enumerate(zip(enhanced_compliance_rules, embeddings))\n",
    "]\n",
    "\n",
    "print(f\"Created {len(points)} points for upsert\")\n",
    "\n",
    "# Upsert points\n",
    "try:\n",
    "    qdrant_client.upsert(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points=points,\n",
    "        wait=True\n",
    "    )\n",
    "    print(f\"✅ Successfully upserted {len(points)} points\")\n",
    "    \n",
    "    # Verify collection has points\n",
    "    collection_info = qdrant_client.get_collection(COLLECTION_NAME)\n",
    "    points_count = collection_info.points_count\n",
    "    print(f\"Collection now contains {points_count} points\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Upsert failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enhanced RAG search function defined\n"
     ]
    }
   ],
   "source": [
    "# Define the enhanced_rag_search_fixed function\n",
    "def enhanced_rag_search_fixed(query: str, top_k: int = 3, categories: List[str] = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Enhanced RAG search with fixed filters and error handling\"\"\"\n",
    "    print(f\"\\n🔍 Enhanced RAG search for: '{query}'\")\n",
    "    \n",
    "    try:\n",
    "        # Build filter if categories specified\n",
    "        query_filter = None\n",
    "        if categories:\n",
    "            query_filter = build_filter(\n",
    "                categories=categories,\n",
    "                severity=[\"high\", \"medium\"]  # Focus on important rules\n",
    "            )\n",
    "            print(f\"Applied filter for categories: {categories}\")\n",
    "        \n",
    "        # Use backend search function\n",
    "        results = search_dense_by_text(\n",
    "            client=qdrant_client,\n",
    "            name=COLLECTION_NAME,\n",
    "            query_text=query,\n",
    "            limit=top_k,\n",
    "            query_filter=query_filter,\n",
    "            openai_client=openai_client,\n",
    "            embed_model=EMBED_MODEL\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Found {len(results)} results\")\n",
    "        \n",
    "        # Format results\n",
    "        formatted_results = []\n",
    "        for i, hit in enumerate(results, 1):\n",
    "            result = {\n",
    "                \"text\": hit.payload[\"text\"],\n",
    "                \"category\": hit.payload[\"category\"],\n",
    "                \"severity\": hit.payload[\"severity\"],\n",
    "                \"regulation\": hit.payload.get(\"regulation\", \"unknown\"),\n",
    "                \"score\": hit.score\n",
    "            }\n",
    "            formatted_results.append(result)\n",
    "            \n",
    "            print(f\"   {i}. Score: {hit.score:.3f} | Category: {result['category']} | Severity: {result['severity']}\")\n",
    "            print(f\"      Text: {result['text'][:100]}...\")\n",
    "            print(f\"      Regulation: {result['regulation']}\")\n",
    "        \n",
    "        return formatted_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ RAG search failed: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"✅ Enhanced RAG search function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing RAG search functionality...\n",
      "\n",
      "🔍 Enhanced RAG search for: 'portfolio concentration risk'\n",
      "Applied filter for categories: ['concentration_limits']\n",
      "✅ Found 1 results\n",
      "   1. Score: 0.578 | Category: concentration_limits | Severity: high\n",
      "      Text: No single equity position shall exceed 25% of total portfolio value to maintain diversification and ...\n",
      "      Regulation: basel_iii\n",
      "\n",
      "🔍 Enhanced RAG search for: 'technology sector exposure limits'\n",
      "Applied filter for categories: ['sector_limits']\n",
      "✅ Found 1 results\n",
      "   1. Score: 0.642 | Category: sector_limits | Severity: medium\n",
      "      Text: Technology sector allocation should not exceed 35% of total portfolio to avoid overexposure to secto...\n",
      "      Regulation: internal_policy\n",
      "\n",
      "🔍 Enhanced RAG search for: 'ESG sustainable investing requirements'\n",
      "Applied filter for categories: ['esg_compliance']\n",
      "✅ Found 1 results\n",
      "   1. Score: 0.769 | Category: esg_compliance | Severity: high\n",
      "      Text: ESG score below 6.0 requires additional review and justification for inclusion in sustainable portfo...\n",
      "      Regulation: eu_taxonomy\n",
      "\n",
      "🔍 Enhanced RAG search for: 'cash allocation liquidity management'\n",
      "✅ Found 2 results\n",
      "   1. Score: 0.710 | Category: liquidity | Severity: medium\n",
      "      Text: Cash allocation should remain between 5-15% for liquidity management and opportunistic investments d...\n",
      "      Regulation: internal_policy\n",
      "   2. Score: 0.326 | Category: sector_limits | Severity: medium\n",
      "      Text: Technology sector allocation should not exceed 35% of total portfolio to avoid overexposure to secto...\n",
      "      Regulation: internal_policy\n",
      "\n",
      "✅ RAG Search Testing Complete - Ready for agentic system!\n"
     ]
    }
   ],
   "source": [
    "# Test RAG search functionality\n",
    "test_queries = [\n",
    "    (\"portfolio concentration risk\", [\"concentration_limits\"]),\n",
    "    (\"technology sector exposure limits\", [\"sector_limits\"]),\n",
    "    (\"ESG sustainable investing requirements\", [\"esg_compliance\"]),\n",
    "    (\"cash allocation liquidity management\", None),  # No category filter\n",
    "]\n",
    "\n",
    "print(\"🧪 Testing RAG search functionality...\")\n",
    "for query, categories in test_queries:\n",
    "    results = enhanced_rag_search_fixed(query, categories=categories, top_k=2)\n",
    "\n",
    "print(\"\\n✅ RAG Search Testing Complete - Ready for agentic system!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic System with RAG + Letta Memory Integration\n",
    "\n",
    "The following section demonstrates a complete agentic system that combines:\n",
    "- **RAG (Retrieval-Augmented Generation)**: Using Qdrant for compliance rule retrieval\n",
    "- **Letta Memory**: For persistent agent memory and learning  \n",
    "- **LangGraph**: For orchestrating multi-step agent workflows\n",
    "- **OpenTelemetry + LangSmith**: For tracing and observability\n",
    "\n",
    "This showcases how modern AI agent architectures integrate multiple components for sophisticated reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Backend modules imported successfully\n",
      "✅ LangGraph imported successfully\n",
      "✅ OTLP exporter configured for endpoint: http://localhost:4317\n",
      "✅ LangSmith client initialized\n",
      "✅ OpenTelemetry + LangSmith tracing configured\n",
      "✅ Telemetry initialized\n"
     ]
    }
   ],
   "source": [
    "# Import additional modules for agentic system\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "from pydantic import BaseModel, Field\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "import asyncio\n",
    "from contextlib import nullcontext\n",
    "\n",
    "# Backend modules for agent system\n",
    "try:\n",
    "    from backend.app.memory import save, recall, health as memory_health, list_agents\n",
    "    from backend.app.telemetry import setup_telemetry\n",
    "    from backend.app.state import AppState\n",
    "    BACKEND_AVAILABLE = True\n",
    "    print(\"✅ Backend modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ Backend modules not available: {e}\")\n",
    "    BACKEND_AVAILABLE = False\n",
    "\n",
    "# LangGraph imports\n",
    "try:\n",
    "    from langgraph.graph import StateGraph, END\n",
    "    from langgraph.checkpoint.memory import MemorySaver\n",
    "    LANGGRAPH_AVAILABLE = True\n",
    "    print(\"✅ LangGraph imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ LangGraph not available: {e}\")\n",
    "    LANGGRAPH_AVAILABLE = False\n",
    "\n",
    "# Setup telemetry - FIXED: unpack the tuple\n",
    "if BACKEND_AVAILABLE:\n",
    "    try:\n",
    "        tracer, langsmith_client = setup_telemetry()  # Unpack tuple correctly\n",
    "        print(\"✅ Telemetry initialized\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Telemetry setup failed: {e}\")\n",
    "        tracer = None\n",
    "        langsmith_client = None\n",
    "else:\n",
    "    tracer = None\n",
    "    langsmith_client = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pydantic v2 state models defined\n",
      "\n",
      "📊 Memory Service Status:\n",
      "   Letta available: True\n",
      "   Use Letta: True\n",
      "   Service OK: True\n",
      "   Base URL: http://localhost:8283\n"
     ]
    }
   ],
   "source": [
    "# Define Pydantic v2 state models for the agentic system\n",
    "class AgentMemoryItem(BaseModel):\n",
    "    \"\"\"Represents a memory item for the agent\"\"\"\n",
    "    text: str\n",
    "    category: str = \"general\"\n",
    "    timestamp: datetime = Field(default_factory=datetime.now)\n",
    "    metadata: Dict[str, Any] = Field(default_factory=dict)\n",
    "\n",
    "class PortfolioAnalysisRequest(BaseModel):\n",
    "    \"\"\"Request for portfolio analysis\"\"\"\n",
    "    user_query: str\n",
    "    portfolio_data: Optional[Dict[str, Any]] = None\n",
    "    compliance_categories: List[str] = Field(default_factory=list)\n",
    "    user_id: str = Field(default_factory=lambda: f\"user_{uuid.uuid4().hex[:8]}\")\n",
    "\n",
    "class AgentState(BaseModel):\n",
    "    \"\"\"Enhanced agent state with RAG and memory integration\"\"\"\n",
    "    # User input\n",
    "    user_query: str\n",
    "    user_id: str\n",
    "    portfolio_data: Optional[Dict[str, Any]] = None\n",
    "    \n",
    "    # Processing state\n",
    "    compliance_rules: List[Dict[str, Any]] = Field(default_factory=list)\n",
    "    relevant_memories: List[Dict[str, Any]] = Field(default_factory=list)\n",
    "    analysis_result: Optional[str] = None\n",
    "    recommendations: List[str] = Field(default_factory=list)\n",
    "    \n",
    "    # Metadata\n",
    "    session_id: str = Field(default_factory=lambda: f\"session_{uuid.uuid4().hex[:8]}\")\n",
    "    step_count: int = 0\n",
    "    total_cost: float = 0.0\n",
    "    error_message: Optional[str] = None\n",
    "\n",
    "    class Config:\n",
    "        # Pydantic v2 configuration\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "print(\"✅ Pydantic v2 state models defined\")\n",
    "\n",
    "# Check memory service health\n",
    "if BACKEND_AVAILABLE:\n",
    "    memory_status = memory_health()\n",
    "    print(f\"\\n📊 Memory Service Status:\")\n",
    "    print(f\"   Letta available: {memory_status['letta_available']}\")\n",
    "    print(f\"   Use Letta: {memory_status['use_letta']}\")\n",
    "    print(f\"   Service OK: {memory_status['ok']}\")\n",
    "    print(f\"   Base URL: {memory_status['base_url']}\")\n",
    "    if memory_status.get('error'):\n",
    "        print(f\"   Error: {memory_status['error']}\")\n",
    "else:\n",
    "    print(\"⚠️ Memory service status check skipped (backend not available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Agent tools defined with deterministic compliance analysis\n"
     ]
    }
   ],
   "source": [
    "# Agent Tools: RAG + Memory Integration Functions\n",
    "\n",
    "def rag_retrieval_tool(query: str, categories: List[str] = None, top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "    \"\"\"RAG tool for retrieving compliance rules\"\"\"\n",
    "    print(f\"🔍 RAG Retrieval: '{query}' (categories: {categories})\")\n",
    "    \n",
    "    try:\n",
    "        return enhanced_rag_search_fixed(query, top_k=top_k, categories=categories)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ RAG retrieval failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def memory_recall_tool(user_id: str, query: str, k: int = 3) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Memory tool for recalling relevant past experiences\"\"\"\n",
    "    print(f\"🧠 Memory Recall: '{query}' for user {user_id}\")\n",
    "    \n",
    "    if not BACKEND_AVAILABLE:\n",
    "        print(\"⚠️ Backend not available, skipping memory recall\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        memories = recall(user_id, query, k)\n",
    "        print(f\"   Found {len(memories)} relevant memories\")\n",
    "        return memories\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Memory recall failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def memory_save_tool(user_id: str, item: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Memory tool for saving experiences to long-term memory\"\"\"\n",
    "    print(f\"💾 Memory Save: Saving item for user {user_id}\")\n",
    "    \n",
    "    if not BACKEND_AVAILABLE:\n",
    "        print(\"⚠️ Backend not available, skipping memory save\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        success = save(user_id, item)\n",
    "        print(f\"   Save {'successful' if success else 'failed'}\")\n",
    "        return success\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Memory save failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def to_dictish(x):\n",
    "    \"\"\"Convert SDK objects to dicts safely\"\"\"\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return [to_dictish(i) for i in x]\n",
    "    if hasattr(x, \"__dict__\"):\n",
    "        return {k: to_dictish(v) for k, v in x.__dict__.items() if not k.startswith(\"_\")}\n",
    "    if isinstance(x, dict):\n",
    "        return {k: to_dictish(v) for k, v in x.items()}\n",
    "    return x\n",
    "\n",
    "def llm_reasoning_tool(prompt: str, system_msg: str | None = None) -> str:\n",
    "    \"\"\"Fixed LLM reasoning with proper error handling\"\"\"\n",
    "    print(f\"🤖 LLM Reasoning: {len(prompt)} chars\")\n",
    "\n",
    "    model = os.getenv(\"OPENAI_MODEL\", \"gpt-5-nano\")\n",
    "    max_cc = int(os.getenv(\"OPENAI_MAX_TOKENS\", \"1536\"))\n",
    "\n",
    "    messages = []\n",
    "    if system_msg:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_msg})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    try:\n",
    "        # Try Chat Completions first\n",
    "        cc = openai_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            max_completion_tokens=max_cc\n",
    "        )\n",
    "        choice = cc.choices[0]\n",
    "        text = (getattr(choice.message, \"content\", None) or \"\").strip()\n",
    "        finish_reason = getattr(choice, \"finish_reason\", None)\n",
    "\n",
    "        # If truncated or empty, try a shorter prompt\n",
    "        if not text or finish_reason == \"length\":\n",
    "            print(f\"   Chat completion truncated/empty (finish_reason={finish_reason}); retrying with shorter prompt\")\n",
    "            # Try with a much shorter prompt\n",
    "            short_prompt = prompt[:500] + \"...\" if len(prompt) > 500 else prompt\n",
    "            short_messages = []\n",
    "            if system_msg:\n",
    "                short_messages.append({\"role\": \"system\", \"content\": system_msg})\n",
    "            short_messages.append({\"role\": \"user\", \"content\": short_prompt})\n",
    "            \n",
    "            cont = openai_client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=short_messages,\n",
    "                max_completion_tokens=max_cc\n",
    "            )\n",
    "            text = (cont.choices[0].message.content or \"\").strip()\n",
    "\n",
    "        if not text:\n",
    "            # Return a structured fallback analysis based on the retrieved rules\n",
    "            return \"Based on the retrieved compliance rules, a detailed analysis is required to determine compliance status. Please review the specific rules and thresholds mentioned in the context.\"\n",
    "\n",
    "        print(f\"   Generated {len(text)} chars\")\n",
    "        return text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ LLM reasoning failed: {e}\")\n",
    "        # Return a basic compliance analysis fallback\n",
    "        return f\"Analysis unavailable due to technical error: {e}. Please review compliance rules manually.\"\n",
    "\n",
    "# Structured constraint checking with dataclass\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "\n",
    "@dataclass\n",
    "class Constraint:\n",
    "    scope: Literal[\"sector\", \"issuer\", \"asset_class\", \"liquidity\", \"esg\"]\n",
    "    key: str\n",
    "    comparator: Literal[\"<\", \"<=\", \">\", \">=\"]\n",
    "    limit_bps: int\n",
    "    source: str\n",
    "    severity: Literal[\"low\", \"medium\", \"high\", \"blocker\"]\n",
    "\n",
    "def parse_constraint(rule_text: str, category: str, severity: str) -> Optional[Constraint]:\n",
    "    \"\"\"Parse natural language compliance rule into structured constraint\"\"\"\n",
    "    text_lower = rule_text.lower()\n",
    "    \n",
    "    # Sector allocation rules\n",
    "    if \"technology sector\" in text_lower and \"not exceed\" in text_lower:\n",
    "        match = re.search(r\"(\\d+)%\", rule_text)\n",
    "        if match:\n",
    "            limit_pct = int(match.group(1))\n",
    "            return Constraint(\n",
    "                scope=\"sector\",\n",
    "                key=\"technology\",\n",
    "                comparator=\"<=\",\n",
    "                limit_bps=limit_pct * 100,  # Convert to basis points\n",
    "                source=rule_text,\n",
    "                severity=severity\n",
    "            )\n",
    "    \n",
    "    # Individual position limits\n",
    "    if \"single equity position\" in text_lower and \"not exceed\" in text_lower:\n",
    "        match = re.search(r\"(\\d+)%\", rule_text)\n",
    "        if match:\n",
    "            limit_pct = int(match.group(1))\n",
    "            return Constraint(\n",
    "                scope=\"issuer\",\n",
    "                key=\"any_single_position\",\n",
    "                comparator=\"<=\",\n",
    "                limit_bps=limit_pct * 100,\n",
    "                source=rule_text,\n",
    "                severity=severity\n",
    "            )\n",
    "    \n",
    "    # Cryptocurrency limits\n",
    "    if \"cryptocurrency\" in text_lower and \"not exceed\" in text_lower:\n",
    "        match = re.search(r\"(\\d+)%\", rule_text)\n",
    "        if match:\n",
    "            limit_pct = int(match.group(1))\n",
    "            return Constraint(\n",
    "                scope=\"asset_class\",\n",
    "                key=\"cryptocurrency\",\n",
    "                comparator=\"<=\",\n",
    "                limit_bps=limit_pct * 100,\n",
    "                source=rule_text,\n",
    "                severity=severity\n",
    "            )\n",
    "    \n",
    "    # Cash allocation ranges\n",
    "    if \"cash allocation\" in text_lower and \"between\" in text_lower:\n",
    "        matches = re.findall(r\"(\\d+)%\", rule_text)\n",
    "        if len(matches) >= 2:\n",
    "            min_pct, max_pct = int(matches[0]), int(matches[1])\n",
    "            return Constraint(\n",
    "                scope=\"liquidity\",\n",
    "                key=\"cash_range\",\n",
    "                comparator=\">=\",  # We'll handle range separately\n",
    "                limit_bps=min_pct * 100,  # Store min for now\n",
    "                source=rule_text,\n",
    "                severity=severity\n",
    "            )\n",
    "    \n",
    "    # ESG score requirements\n",
    "    if \"esg score\" in text_lower and \"below\" in text_lower:\n",
    "        match = re.search(r\"(\\d+\\.?\\d*)\", rule_text)\n",
    "        if match:\n",
    "            score = float(match.group(1))\n",
    "            return Constraint(\n",
    "                scope=\"esg\",\n",
    "                key=\"min_score\",\n",
    "                comparator=\">=\",\n",
    "                limit_bps=int(score * 100),  # Convert to basis points equivalent\n",
    "                source=rule_text,\n",
    "                severity=severity\n",
    "            )\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_portfolio_values(query: str) -> Dict[str, float]:\n",
    "    \"\"\"Extract portfolio allocations from user query\"\"\"\n",
    "    values = {}\n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    # Technology sector exposure\n",
    "    tech_match = re.search(r\"(\\d+)%.*technology\", query_lower)\n",
    "    if tech_match:\n",
    "        values[\"technology_sector\"] = float(tech_match.group(1))\n",
    "    \n",
    "    # Cryptocurrency exposure\n",
    "    crypto_match = re.search(r\"(\\d+)%.*crypto\", query_lower)\n",
    "    if crypto_match:\n",
    "        values[\"cryptocurrency\"] = float(crypto_match.group(1))\n",
    "    \n",
    "    # Cash allocation\n",
    "    cash_match = re.search(r\"(\\d+)%.*cash\", query_lower)\n",
    "    if cash_match:\n",
    "        values[\"cash\"] = float(cash_match.group(1))\n",
    "    \n",
    "    # ESG score\n",
    "    esg_match = re.search(r\"esg score.*?(\\d+\\.?\\d*)\", query_lower)\n",
    "    if esg_match:\n",
    "        values[\"esg_score\"] = float(esg_match.group(1))\n",
    "    \n",
    "    return values\n",
    "\n",
    "def analyze_portfolio_compliance(query: str, compliance_rules: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"Deterministic compliance analysis with structured constraint checking\"\"\"\n",
    "    \n",
    "    # Extract portfolio values from query\n",
    "    portfolio_values = extract_portfolio_values(query)\n",
    "    \n",
    "    if not portfolio_values:\n",
    "        return \"❌ Could not extract portfolio allocations from query. Please specify percentages clearly.\"\n",
    "    \n",
    "    # Parse rules into structured constraints\n",
    "    constraints = []\n",
    "    for rule in compliance_rules:\n",
    "        constraint = parse_constraint(\n",
    "            rule[\"text\"], \n",
    "            rule[\"category\"], \n",
    "            rule[\"severity\"]\n",
    "        )\n",
    "        if constraint:\n",
    "            constraints.append(constraint)\n",
    "    \n",
    "    # Perform compliance checking\n",
    "    violations = []\n",
    "    warnings = []\n",
    "    compliant_items = []\n",
    "    \n",
    "    for key, value in portfolio_values.items():\n",
    "        value_bps = int(value * 100)  # Convert to basis points\n",
    "        \n",
    "        for constraint in constraints:\n",
    "            is_relevant = False\n",
    "            constraint_name = \"\"\n",
    "            \n",
    "            # Check if constraint applies to this portfolio value\n",
    "            if key == \"technology_sector\" and constraint.scope == \"sector\" and constraint.key == \"technology\":\n",
    "                is_relevant = True\n",
    "                constraint_name = \"Technology Sector Limit\"\n",
    "            elif key == \"cryptocurrency\" and constraint.scope == \"asset_class\" and constraint.key == \"cryptocurrency\":\n",
    "                is_relevant = True\n",
    "                constraint_name = \"Cryptocurrency Limit\"\n",
    "            elif key == \"cash\" and constraint.scope == \"liquidity\" and constraint.key == \"cash_range\":\n",
    "                is_relevant = True\n",
    "                constraint_name = \"Cash Allocation Range\"\n",
    "            elif key == \"esg_score\" and constraint.scope == \"esg\" and constraint.key == \"min_score\":\n",
    "                is_relevant = True\n",
    "                constraint_name = \"ESG Score Requirement\"\n",
    "                value_bps = int(value * 100)  # ESG scores in basis points equivalent\n",
    "            \n",
    "            if is_relevant:\n",
    "                limit_value = constraint.limit_bps / 100  # Convert back to percentage\n",
    "                \n",
    "                # Check compliance\n",
    "                if constraint.comparator == \"<=\":\n",
    "                    if value_bps > constraint.limit_bps:\n",
    "                        if constraint.severity == \"high\":\n",
    "                            violations.append(f\"❌ BREACH: {constraint_name} - {value}% > {limit_value}% limit\")\n",
    "                        else:\n",
    "                            warnings.append(f\"⚠️ WARNING: {constraint_name} - {value}% > {limit_value}% limit\")\n",
    "                    elif value_bps == constraint.limit_bps:\n",
    "                        warnings.append(f\"⚠️ AT LIMIT: {constraint_name} - {value}% exactly at {limit_value}% limit\")\n",
    "                    else:\n",
    "                        margin = limit_value - value\n",
    "                        compliant_items.append(f\"✅ COMPLIANT: {constraint_name} - {value}% (margin: {margin:.1f}%)\")\n",
    "                \n",
    "                elif constraint.comparator == \">=\" and constraint.scope == \"esg\":\n",
    "                    if value_bps < constraint.limit_bps:\n",
    "                        if constraint.severity == \"high\":\n",
    "                            violations.append(f\"❌ BREACH: {constraint_name} - {value} < {limit_value} minimum\")\n",
    "                        else:\n",
    "                            warnings.append(f\"⚠️ WARNING: {constraint_name} - {value} < {limit_value} minimum\")\n",
    "                    else:\n",
    "                        compliant_items.append(f\"✅ COMPLIANT: {constraint_name} - {value} >= {limit_value}\")\n",
    "                \n",
    "                # Special handling for cash range\n",
    "                elif constraint.scope == \"liquidity\" and constraint.key == \"cash_range\":\n",
    "                    # Extract range from source text\n",
    "                    matches = re.findall(r\"(\\d+)%\", constraint.source)\n",
    "                    if len(matches) >= 2:\n",
    "                        min_cash, max_cash = float(matches[0]), float(matches[1])\n",
    "                        if value < min_cash:\n",
    "                            warnings.append(f\"⚠️ WARNING: Cash allocation {value}% below minimum {min_cash}%\")\n",
    "                        elif value > max_cash:\n",
    "                            warnings.append(f\"⚠️ WARNING: Cash allocation {value}% above maximum {max_cash}%\")\n",
    "                        else:\n",
    "                            compliant_items.append(f\"✅ COMPLIANT: Cash allocation {value}% within {min_cash}%-{max_cash}% range\")\n",
    "    \n",
    "    # Generate compliance summary\n",
    "    summary_parts = []\n",
    "    summary_parts.append(\"=\" * 50)\n",
    "    summary_parts.append(\"📊 COMPLIANCE SUMMARY\")\n",
    "    summary_parts.append(\"=\" * 50)\n",
    "    \n",
    "    # Portfolio overview\n",
    "    summary_parts.append(\"\\n🎯 Portfolio Allocations:\")\n",
    "    for key, value in portfolio_values.items():\n",
    "        formatted_key = key.replace(\"_\", \" \").title()\n",
    "        if \"score\" in key.lower():\n",
    "            summary_parts.append(f\"   • {formatted_key}: {value}\")\n",
    "        else:\n",
    "            summary_parts.append(f\"   • {formatted_key}: {value}%\")\n",
    "    \n",
    "    # Violations (highest priority)\n",
    "    if violations:\n",
    "        summary_parts.append(f\"\\n🚨 VIOLATIONS ({len(violations)}):\")\n",
    "        for violation in violations:\n",
    "            summary_parts.append(f\"   {violation}\")\n",
    "    \n",
    "    # Warnings\n",
    "    if warnings:\n",
    "        summary_parts.append(f\"\\n⚠️ WARNINGS ({len(warnings)}):\")\n",
    "        for warning in warnings:\n",
    "            summary_parts.append(f\"   {warning}\")\n",
    "    \n",
    "    # Compliant items\n",
    "    if compliant_items:\n",
    "        summary_parts.append(f\"\\n✅ COMPLIANT ITEMS ({len(compliant_items)}):\")\n",
    "        for item in compliant_items:\n",
    "            summary_parts.append(f\"   {item}\")\n",
    "    \n",
    "    # Overall status\n",
    "    summary_parts.append(\"\\n\" + \"=\" * 50)\n",
    "    if violations:\n",
    "        summary_parts.append(\"🔴 OVERALL STATUS: NON-COMPLIANT\")\n",
    "        summary_parts.append(\"⚠️ Action Required: Address violations before proceeding\")\n",
    "    elif warnings:\n",
    "        summary_parts.append(\"🟡 OVERALL STATUS: COMPLIANT WITH WARNINGS\")\n",
    "        summary_parts.append(\"💡 Recommendation: Monitor closely and consider rebalancing\")\n",
    "    else:\n",
    "        summary_parts.append(\"🟢 OVERALL STATUS: FULLY COMPLIANT\")\n",
    "        summary_parts.append(\"✅ Portfolio meets all regulatory requirements\")\n",
    "    \n",
    "    summary_parts.append(\"=\" * 50)\n",
    "    \n",
    "    return \"\\n\".join(summary_parts)\n",
    "\n",
    "print(\"✅ Agent tools defined with deterministic compliance analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LangGraph agent nodes defined with deterministic fallback\n"
     ]
    }
   ],
   "source": [
    "# LangGraph Agent Nodes with Telemetry\n",
    "\n",
    "def recall_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Node 1: Recall relevant memories from past interactions\"\"\"\n",
    "    print(f\"\\n🔄 Step {state.step_count + 1}: Memory Recall Node\")\n",
    "    \n",
    "    # Start telemetry span\n",
    "    span_name = f\"agent.recall\"\n",
    "    with tracer.start_as_current_span(span_name) if tracer else nullcontext():\n",
    "        try:\n",
    "            # Recall relevant memories\n",
    "            memories = memory_recall_tool(\n",
    "                user_id=state.user_id,\n",
    "                query=state.user_query,\n",
    "                k=3\n",
    "            )\n",
    "            \n",
    "            # Update state\n",
    "            new_state = state.model_copy(deep=True)\n",
    "            new_state.relevant_memories = memories\n",
    "            new_state.step_count += 1\n",
    "            \n",
    "            print(f\"   ✅ Recalled {len(memories)} memories\")\n",
    "            return new_state\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Recall node failed: {e}\")\n",
    "            new_state = state.model_copy(deep=True)\n",
    "            new_state.error_message = f\"Recall failed: {e}\"\n",
    "            new_state.step_count += 1\n",
    "            return new_state\n",
    "\n",
    "def retrieve_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Node 2: Retrieve relevant compliance rules using RAG\"\"\"\n",
    "    print(f\"\\n🔄 Step {state.step_count + 1}: RAG Retrieval Node\")\n",
    "    \n",
    "    span_name = f\"agent.retrieve\"\n",
    "    with tracer.start_as_current_span(span_name) if tracer else nullcontext():\n",
    "        try:\n",
    "            # Extract categories from query or use all\n",
    "            categories = None  # Could be extracted from query analysis\n",
    "            \n",
    "            # Retrieve compliance rules\n",
    "            rules = rag_retrieval_tool(\n",
    "                query=state.user_query,\n",
    "                categories=categories,\n",
    "                top_k=5\n",
    "            )\n",
    "            \n",
    "            # Update state\n",
    "            new_state = state.model_copy(deep=True)\n",
    "            new_state.compliance_rules = rules\n",
    "            new_state.step_count += 1\n",
    "            \n",
    "            print(f\"   ✅ Retrieved {len(rules)} compliance rules\")\n",
    "            return new_state\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Retrieve node failed: {e}\")\n",
    "            new_state = state.model_copy(deep=True)\n",
    "            new_state.error_message = f\"Retrieval failed: {e}\"\n",
    "            new_state.step_count += 1\n",
    "            return new_state\n",
    "\n",
    "def analyze_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Node 3: Analyze using LLM with RAG and memory context + deterministic fallback\"\"\"\n",
    "    print(f\"\\n🔄 Step {state.step_count + 1}: Analysis Node\")\n",
    "    \n",
    "    span_name = f\"agent.analyze\"\n",
    "    with tracer.start_as_current_span(span_name) if tracer else nullcontext():\n",
    "        try:\n",
    "            # First try deterministic compliance analysis\n",
    "            deterministic_analysis = analyze_portfolio_compliance(state.user_query, state.compliance_rules)\n",
    "            print(f\"   ✅ Generated deterministic analysis ({len(deterministic_analysis)} chars)\")\n",
    "            \n",
    "            # Build context from RAG and memory for LLM enhancement\n",
    "            context_parts = []\n",
    "            \n",
    "            if state.compliance_rules:\n",
    "                rules_text = \"\\n\".join([\n",
    "                    f\"- {rule['text']} (Category: {rule['category']}, Severity: {rule['severity']})\"\n",
    "                    for rule in state.compliance_rules[:3]\n",
    "                ])\n",
    "                context_parts.append(f\"Relevant Compliance Rules:\\n{rules_text}\")\n",
    "            \n",
    "            if state.relevant_memories:\n",
    "                memories_text = \"\\n\".join([\n",
    "                    f\"- {mem.get('text', str(mem))}\"\n",
    "                    for mem in state.relevant_memories[:2]\n",
    "                ])\n",
    "                context_parts.append(f\"Relevant Past Experiences:\\n{memories_text}\")\n",
    "            \n",
    "            context = \"\\n\\n\".join(context_parts)\n",
    "            \n",
    "            # Try to enhance with LLM analysis\n",
    "            analysis_prompt = f\"\"\"\n",
    "You are a portfolio compliance advisor. I have already performed a deterministic compliance check.\n",
    "Please provide additional insights and recommendations based on the context.\n",
    "\n",
    "User Query: {state.user_query}\n",
    "\n",
    "Deterministic Analysis Results:\n",
    "{deterministic_analysis}\n",
    "\n",
    "Additional Context:\n",
    "{context}\n",
    "\n",
    "Please provide:\n",
    "1. Validation of the deterministic analysis\n",
    "2. Additional risk considerations\n",
    "3. Actionable recommendations\n",
    "4. Next steps\n",
    "\n",
    "Keep it concise and practical.\n",
    "\"\"\"\n",
    "            \n",
    "            system_msg = \"You are an expert portfolio compliance advisor. Provide practical, actionable advice.\"\n",
    "            \n",
    "            # Try LLM enhancement\n",
    "            try:\n",
    "                llm_enhancement = llm_reasoning_tool(analysis_prompt, system_msg)\n",
    "                combined_analysis = f\"{deterministic_analysis}\\n\\n--- ADDITIONAL INSIGHTS ---\\n{llm_enhancement}\"\n",
    "                print(f\"   ✅ Enhanced analysis with LLM ({len(llm_enhancement)} additional chars)\")\n",
    "            except Exception as llm_error:\n",
    "                print(f\"   ⚠️ LLM enhancement failed, using deterministic analysis: {llm_error}\")\n",
    "                combined_analysis = deterministic_analysis\n",
    "            \n",
    "            # Extract recommendations from the analysis\n",
    "            recommendations = []\n",
    "            analysis_lower = combined_analysis.lower()\n",
    "            \n",
    "            # Look for action items and recommendations\n",
    "            lines = combined_analysis.split('\\n')\n",
    "            for line in lines:\n",
    "                line_lower = line.lower().strip()\n",
    "                if any(word in line_lower for word in [\"recommend\", \"should\", \"must\", \"consider\", \"target:\", \"action required\"]):\n",
    "                    if line.strip() and len(line.strip()) > 10:  # Avoid very short lines\n",
    "                        recommendations.append(line.strip())\n",
    "            \n",
    "            # Update state\n",
    "            new_state = state.model_copy(deep=True)\n",
    "            new_state.analysis_result = combined_analysis\n",
    "            new_state.recommendations = recommendations[:5]  # Limit to top 5\n",
    "            new_state.step_count += 1\n",
    "            \n",
    "            print(f\"   ✅ Generated combined analysis ({len(combined_analysis)} chars)\")\n",
    "            print(f\"   ✅ Extracted {len(recommendations)} recommendations\")\n",
    "            return new_state\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Analysis node failed: {e}\")\n",
    "            # Fallback to basic deterministic analysis\n",
    "            try:\n",
    "                fallback_analysis = analyze_portfolio_compliance(state.user_query, state.compliance_rules)\n",
    "                new_state = state.model_copy(deep=True)\n",
    "                new_state.analysis_result = fallback_analysis\n",
    "                new_state.recommendations = [\"Review compliance requirements\", \"Consult with risk management\"]\n",
    "                new_state.step_count += 1\n",
    "                print(f\"   ✅ Used fallback deterministic analysis\")\n",
    "                return new_state\n",
    "            except Exception as fallback_error:\n",
    "                new_state = state.model_copy(deep=True)\n",
    "                new_state.error_message = f\"Analysis failed: {e}, Fallback failed: {fallback_error}\"\n",
    "                new_state.step_count += 1\n",
    "                return new_state\n",
    "\n",
    "def memorize_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Node 4: Save the interaction to long-term memory\"\"\"\n",
    "    print(f\"\\n🔄 Step {state.step_count + 1}: Memorization Node\")\n",
    "    \n",
    "    span_name = f\"agent.memorize\"\n",
    "    with tracer.start_as_current_span(span_name) if tracer else nullcontext():\n",
    "        try:\n",
    "            # Create memory item\n",
    "            memory_item = {\n",
    "                \"query\": state.user_query,\n",
    "                \"analysis\": state.analysis_result,\n",
    "                \"recommendations\": state.recommendations,\n",
    "                \"compliance_rules_used\": len(state.compliance_rules),\n",
    "                \"session_id\": state.session_id,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"metadata\": {\n",
    "                    \"type\": \"portfolio_analysis\",\n",
    "                    \"step_count\": state.step_count,\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Save to memory\n",
    "            success = memory_save_tool(state.user_id, memory_item)\n",
    "            \n",
    "            # Update state\n",
    "            new_state = state.model_copy(deep=True)\n",
    "            new_state.step_count += 1\n",
    "            \n",
    "            print(f\"   ✅ Memory save {'successful' if success else 'failed'}\")\n",
    "            return new_state\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Memorize node failed: {e}\")\n",
    "            new_state = state.model_copy(deep=True)\n",
    "            new_state.error_message = f\"Memorization failed: {e}\"\n",
    "            new_state.step_count += 1\n",
    "            return new_state\n",
    "\n",
    "print(\"✅ LangGraph agent nodes defined with deterministic fallback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LangGraph agent nodes defined\n"
     ]
    }
   ],
   "source": [
    "# LangGraph Agent Nodes with Telemetry\n",
    "\n",
    "def recall_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Node 1: Recall relevant memories from past interactions\"\"\"\n",
    "    print(f\"\\n🔄 Step {state.step_count + 1}: Memory Recall Node\")\n",
    "    \n",
    "    # Start telemetry span\n",
    "    span_name = f\"agent.recall\"\n",
    "    with tracer.start_as_current_span(span_name) if tracer else nullcontext():\n",
    "        try:\n",
    "            # Recall relevant memories\n",
    "            memories = memory_recall_tool(\n",
    "                user_id=state.user_id,\n",
    "                query=state.user_query,\n",
    "                k=3\n",
    "            )\n",
    "            \n",
    "            # Update state\n",
    "            new_state = state.model_copy(deep=True)\n",
    "            new_state.relevant_memories = memories\n",
    "            new_state.step_count += 1\n",
    "            \n",
    "            print(f\"   ✅ Recalled {len(memories)} memories\")\n",
    "            return new_state\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Recall node failed: {e}\")\n",
    "            new_state = state.model_copy(deep=True)\n",
    "            new_state.error_message = f\"Recall failed: {e}\"\n",
    "            new_state.step_count += 1\n",
    "            return new_state\n",
    "\n",
    "def retrieve_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Node 2: Retrieve relevant compliance rules using RAG\"\"\"\n",
    "    print(f\"\\n🔄 Step {state.step_count + 1}: RAG Retrieval Node\")\n",
    "    \n",
    "    span_name = f\"agent.retrieve\"\n",
    "    with tracer.start_as_current_span(span_name) if tracer else nullcontext():\n",
    "        try:\n",
    "            # Extract categories from query or use all\n",
    "            categories = None  # Could be extracted from query analysis\n",
    "            \n",
    "            # Retrieve compliance rules\n",
    "            rules = rag_retrieval_tool(\n",
    "                query=state.user_query,\n",
    "                categories=categories,\n",
    "                top_k=5\n",
    "            )\n",
    "            \n",
    "            # Update state\n",
    "            new_state = state.model_copy(deep=True)\n",
    "            new_state.compliance_rules = rules\n",
    "            new_state.step_count += 1\n",
    "            \n",
    "            print(f\"   ✅ Retrieved {len(rules)} compliance rules\")\n",
    "            return new_state\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Retrieve node failed: {e}\")\n",
    "            new_state = state.model_copy(deep=True)\n",
    "            new_state.error_message = f\"Retrieval failed: {e}\"\n",
    "            new_state.step_count += 1\n",
    "            return new_state\n",
    "\n",
    "def analyze_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Node 3: Analyze using LLM with RAG and memory context\"\"\"\n",
    "    print(f\"\\n🔄 Step {state.step_count + 1}: Analysis Node\")\n",
    "    \n",
    "    span_name = f\"agent.analyze\"\n",
    "    with tracer.start_as_current_span(span_name) if tracer else nullcontext():\n",
    "        try:\n",
    "            # Build context from RAG and memory\n",
    "            context_parts = []\n",
    "            \n",
    "            if state.compliance_rules:\n",
    "                rules_text = \"\\n\".join([\n",
    "                    f\"- {rule['text']} (Category: {rule['category']}, Severity: {rule['severity']})\"\n",
    "                    for rule in state.compliance_rules[:3]\n",
    "                ])\n",
    "                context_parts.append(f\"Relevant Compliance Rules:\\n{rules_text}\")\n",
    "            \n",
    "            if state.relevant_memories:\n",
    "                memories_text = \"\\n\".join([\n",
    "                    f\"- {mem.get('text', str(mem))}\"\n",
    "                    for mem in state.relevant_memories[:2]\n",
    "                ])\n",
    "                context_parts.append(f\"Relevant Past Experiences:\\n{memories_text}\")\n",
    "            \n",
    "            context = \"\\n\\n\".join(context_parts)\n",
    "            \n",
    "            # Create analysis prompt\n",
    "            analysis_prompt = f\"\"\"\n",
    "You are a portfolio compliance advisor. Analyze the following query using the provided context.\n",
    "\n",
    "User Query: {state.user_query}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Please provide:\n",
    "1. A clear analysis of any compliance issues\n",
    "2. Specific recommendations based on the rules and past experiences\n",
    "3. Risk assessment and next steps\n",
    "\n",
    "Be concise but thorough.\n",
    "\"\"\"\n",
    "            \n",
    "            system_msg = \"You are an expert portfolio compliance advisor specializing in regulatory analysis and risk management.\"\n",
    "            \n",
    "            # Generate analysis\n",
    "            analysis = llm_reasoning_tool(analysis_prompt, system_msg)\n",
    "            \n",
    "            # Extract recommendations (simplified)\n",
    "            recommendations = []\n",
    "            if \"recommend\" in analysis.lower():\n",
    "                # Simple extraction - could be more sophisticated\n",
    "                lines = analysis.split(\"\\n\")\n",
    "                for line in lines:\n",
    "                    if any(word in line.lower() for word in [\"recommend\", \"should\", \"must\", \"consider\"]):\n",
    "                        recommendations.append(line.strip())\n",
    "            \n",
    "            # Update state\n",
    "            new_state = state.model_copy(deep=True)\n",
    "            new_state.analysis_result = analysis\n",
    "            new_state.recommendations = recommendations[:5]  # Limit to top 5\n",
    "            new_state.step_count += 1\n",
    "            \n",
    "            print(f\"   ✅ Generated analysis ({len(analysis)} chars)\")\n",
    "            print(f\"   ✅ Extracted {len(recommendations)} recommendations\")\n",
    "            return new_state\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Analysis node failed: {e}\")\n",
    "            new_state = state.model_copy(deep=True)\n",
    "            new_state.error_message = f\"Analysis failed: {e}\"\n",
    "            new_state.step_count += 1\n",
    "            return new_state\n",
    "\n",
    "def memorize_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Node 4: Save the interaction to long-term memory\"\"\"\n",
    "    print(f\"\\n🔄 Step {state.step_count + 1}: Memorization Node\")\n",
    "    \n",
    "    span_name = f\"agent.memorize\"\n",
    "    with tracer.start_as_current_span(span_name) if tracer else nullcontext():\n",
    "        try:\n",
    "            # Create memory item\n",
    "            memory_item = {\n",
    "                \"query\": state.user_query,\n",
    "                \"analysis\": state.analysis_result,\n",
    "                \"recommendations\": state.recommendations,\n",
    "                \"compliance_rules_used\": len(state.compliance_rules),\n",
    "                \"session_id\": state.session_id,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"metadata\": {\n",
    "                    \"type\": \"portfolio_analysis\",\n",
    "                    \"step_count\": state.step_count,\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Save to memory\n",
    "            success = memory_save_tool(state.user_id, memory_item)\n",
    "            \n",
    "            # Update state\n",
    "            new_state = state.model_copy(deep=True)\n",
    "            new_state.step_count += 1\n",
    "            \n",
    "            print(f\"   ✅ Memory save {'successful' if success else 'failed'}\")\n",
    "            return new_state\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Memorize node failed: {e}\")\n",
    "            new_state = state.model_copy(deep=True)\n",
    "            new_state.error_message = f\"Memorization failed: {e}\"\n",
    "            new_state.step_count += 1\n",
    "            return new_state\n",
    "\n",
    "print(\"✅ LangGraph agent nodes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Building LangGraph workflow...\n",
      "✅ LangGraph workflow compiled successfully\n",
      "🎯 Agent graph ready for execution\n"
     ]
    }
   ],
   "source": [
    "# Build LangGraph Workflow\n",
    "def create_agent_graph():\n",
    "    \"\"\"Create the LangGraph workflow for the agentic system\"\"\"\n",
    "    print(\"🔧 Building LangGraph workflow...\")\n",
    "    \n",
    "    if not LANGGRAPH_AVAILABLE:\n",
    "        print(\"❌ LangGraph not available, returning None\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Create the graph\n",
    "        workflow = StateGraph(AgentState)\n",
    "        \n",
    "        # Add nodes\n",
    "        workflow.add_node(\"recall\", recall_node)\n",
    "        workflow.add_node(\"retrieve\", retrieve_node) \n",
    "        workflow.add_node(\"analyze\", analyze_node)\n",
    "        workflow.add_node(\"memorize\", memorize_node)\n",
    "        \n",
    "        # Define the flow: recall -> retrieve -> analyze -> memorize\n",
    "        workflow.set_entry_point(\"recall\")\n",
    "        workflow.add_edge(\"recall\", \"retrieve\")\n",
    "        workflow.add_edge(\"retrieve\", \"analyze\")\n",
    "        workflow.add_edge(\"analyze\", \"memorize\")\n",
    "        workflow.add_edge(\"memorize\", END)\n",
    "        \n",
    "        # Add memory checkpointer for state persistence\n",
    "        memory = MemorySaver()\n",
    "        \n",
    "        # Compile the graph\n",
    "        graph = workflow.compile(checkpointer=memory)\n",
    "        \n",
    "        print(\"✅ LangGraph workflow compiled successfully\")\n",
    "        return graph\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to create graph: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create the agent graph\n",
    "agent_graph = create_agent_graph()\n",
    "\n",
    "if agent_graph:\n",
    "    print(\"🎯 Agent graph ready for execution\")\n",
    "else:\n",
    "    print(\"⚠️ Agent graph creation failed - will run simplified version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Testing framework ready\n"
     ]
    }
   ],
   "source": [
    "# Test the Complete Agentic System\n",
    "\n",
    "def run_agentic_analysis(user_query: str, user_id: str = None) -> AgentState:\n",
    "    \"\"\"Run the complete agentic analysis workflow\"\"\"\n",
    "    \n",
    "    # Create initial state\n",
    "    if not user_id:\n",
    "        user_id = f\"demo_user_{uuid.uuid4().hex[:6]}\"\n",
    "    \n",
    "    initial_state = AgentState(\n",
    "        user_query=user_query,\n",
    "        user_id=user_id\n",
    "    )\n",
    "    \n",
    "    print(f\"🚀 Starting agentic analysis for user: {user_id}\")\n",
    "    print(f\"Query: {user_query}\")\n",
    "    print(f\"Session: {initial_state.session_id}\")\n",
    "    \n",
    "    if agent_graph:\n",
    "        # Use LangGraph workflow\n",
    "        try:\n",
    "            print(\"\\n📊 Using LangGraph workflow...\")\n",
    "            \n",
    "            # Run the graph\n",
    "            config = {\"configurable\": {\"thread_id\": initial_state.session_id}}\n",
    "            final_state = agent_graph.invoke(initial_state, config=config)\n",
    "            \n",
    "            # Convert dict result back to AgentState if needed\n",
    "            if isinstance(final_state, dict):\n",
    "                print(\"   🔄 Converting dict result to AgentState...\")\n",
    "                final_state = AgentState(**final_state)\n",
    "            \n",
    "            return final_state\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ LangGraph execution failed: {e}\")\n",
    "            print(\"🔄 Falling back to manual execution...\")\n",
    "            \n",
    "    # Fallback: Manual execution\n",
    "    print(\"\\n🔄 Using manual step-by-step execution...\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Recall\n",
    "        state = recall_node(initial_state)\n",
    "        \n",
    "        # Step 2: Retrieve\n",
    "        state = retrieve_node(state)\n",
    "        \n",
    "        # Step 3: Analyze\n",
    "        state = analyze_node(state)\n",
    "        \n",
    "        # Step 4: Memorize\n",
    "        state = memorize_node(state)\n",
    "        \n",
    "        return state\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Manual execution failed: {e}\")\n",
    "        error_state = initial_state.model_copy(deep=True)\n",
    "        error_state.error_message = f\"Execution failed: {e}\"\n",
    "        return error_state\n",
    "\n",
    "def display_results(state):\n",
    "    \"\"\"Display the results of the agentic analysis - handles both dict and AgentState objects\"\"\"\n",
    "    \n",
    "    # Handle both dict and AgentState objects\n",
    "    if isinstance(state, dict):\n",
    "        # Extract values from dict\n",
    "        session_id = state.get('session_id', 'unknown')\n",
    "        user_id = state.get('user_id', 'unknown')\n",
    "        step_count = state.get('step_count', 0)\n",
    "        error_message = state.get('error_message')\n",
    "        compliance_rules = state.get('compliance_rules', [])\n",
    "        relevant_memories = state.get('relevant_memories', [])\n",
    "        analysis_result = state.get('analysis_result')\n",
    "        recommendations = state.get('recommendations', [])\n",
    "    else:\n",
    "        # AgentState object\n",
    "        session_id = state.session_id\n",
    "        user_id = state.user_id\n",
    "        step_count = state.step_count\n",
    "        error_message = state.error_message\n",
    "        compliance_rules = state.compliance_rules\n",
    "        relevant_memories = state.relevant_memories\n",
    "        analysis_result = state.analysis_result\n",
    "        recommendations = state.recommendations\n",
    "    \n",
    "    print(f\"\\n🎯 Analysis Results for Session: {session_id}\")\n",
    "    print(f\"User: {user_id}\")\n",
    "    print(f\"Steps completed: {step_count}\")\n",
    "    \n",
    "    if error_message:\n",
    "        print(f\"\\n❌ Error: {error_message}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n📊 Compliance Rules Retrieved: {len(compliance_rules)}\")\n",
    "    for i, rule in enumerate(compliance_rules[:3], 1):\n",
    "        print(f\"   {i}. {rule['category']}: {rule['text'][:80]}... (Score: {rule['score']:.3f})\")\n",
    "    \n",
    "    print(f\"\\n🧠 Memories Recalled: {len(relevant_memories)}\")\n",
    "    for i, mem in enumerate(relevant_memories[:2], 1):\n",
    "        text = mem.get('text', str(mem))[:180]\n",
    "        print(f\"   {i}. {text}...\")\n",
    "    \n",
    "    if analysis_result:\n",
    "        print(f\"\\n📝 Analysis Result:\")\n",
    "        print(f\"   {analysis_result}\")\n",
    "        \n",
    "    print(f\"\\n💡 Recommendations: {len(recommendations)}\")\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"   {i}. {rec}.\")\n",
    "\n",
    "print(\"✅ Testing framework ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎪 Demo 1: Portfolio Concentration Analysis\n",
      "============================================================\n",
      "🚀 Starting agentic analysis for user: portfolio_manager_alice\n",
      "Query: I have 35% of my portfolio in technology stocks. Is this compliant with concentration limits?\n",
      "Session: session_b0c1b917\n",
      "\n",
      "📊 Using LangGraph workflow...\n",
      "\n",
      "🔄 Step 1: Memory Recall Node\n",
      "🧠 Memory Recall: 'I have 35% of my portfolio in technology stocks. Is this compliant with concentration limits?' for user portfolio_manager_alice\n",
      "   Found 3 relevant memories\n",
      "   ✅ Recalled 3 memories\n",
      "\n",
      "🔄 Step 2: RAG Retrieval Node\n",
      "🔍 RAG Retrieval: 'I have 35% of my portfolio in technology stocks. Is this compliant with concentration limits?' (categories: None)\n",
      "\n",
      "🔍 Enhanced RAG search for: 'I have 35% of my portfolio in technology stocks. Is this compliant with concentration limits?'\n",
      "✅ Found 5 results\n",
      "   1. Score: 0.647 | Category: sector_limits | Severity: medium\n",
      "      Text: Technology sector allocation should not exceed 35% of total portfolio to avoid overexposure to secto...\n",
      "      Regulation: internal_policy\n",
      "   2. Score: 0.606 | Category: concentration_limits | Severity: high\n",
      "      Text: No single equity position shall exceed 25% of total portfolio value to maintain diversification and ...\n",
      "      Regulation: basel_iii\n",
      "   3. Score: 0.550 | Category: alternative_assets | Severity: high\n",
      "      Text: Cryptocurrency exposure must not exceed 5% of total portfolio value due to regulatory uncertainty an...\n",
      "      Regulation: sec_guidance\n",
      "   4. Score: 0.409 | Category: liquidity | Severity: medium\n",
      "      Text: Cash allocation should remain between 5-15% for liquidity management and opportunistic investments d...\n",
      "      Regulation: internal_policy\n",
      "   5. Score: 0.401 | Category: esg_compliance | Severity: high\n",
      "      Text: ESG score below 6.0 requires additional review and justification for inclusion in sustainable portfo...\n",
      "      Regulation: eu_taxonomy\n",
      "   ✅ Retrieved 5 compliance rules\n",
      "\n",
      "🔄 Step 3: Analysis Node\n",
      "🤖 LLM Reasoning: 1022 chars\n",
      "   Chat completion truncated/empty (finish_reason=length); retrying with shorter prompt\n",
      "   Generated 1293 chars\n",
      "   ✅ Generated analysis (1293 chars)\n",
      "   ✅ Extracted 3 recommendations\n",
      "\n",
      "🔄 Step 4: Memorization Node\n",
      "💾 Memory Save: Saving item for user portfolio_manager_alice\n",
      "   Save successful\n",
      "   ✅ Memory save successful\n",
      "   🔄 Converting dict result to AgentState...\n",
      "\n",
      "🎯 Analysis Results for Session: session_b0c1b917\n",
      "User: portfolio_manager_alice\n",
      "Steps completed: 4\n",
      "\n",
      "📊 Compliance Rules Retrieved: 5\n",
      "   1. sector_limits: Technology sector allocation should not exceed 35% of total portfolio to avoid o... (Score: 0.647)\n",
      "   2. concentration_limits: No single equity position shall exceed 25% of total portfolio value to maintain ... (Score: 0.606)\n",
      "   3. alternative_assets: Cryptocurrency exposure must not exceed 5% of total portfolio value due to regul... (Score: 0.550)\n",
      "\n",
      "🧠 Memories Recalled: 3\n",
      "   1. ...\n",
      "   2. ...\n",
      "\n",
      "📝 Analysis Result:\n",
      "   Short answer: Yes, 35% in technology stocks is compliant with the stated sector limit, since the rule says the technology allocation should “not exceed 35%” of the portfolio. Being at 35% exactly meets the limit.\n",
      "\n",
      "However, a couple of caveats to check:\n",
      "- No single equity position: The second rule (no single equity position shall exceed 25% of total portfolio value) could still be violated even if tech as a sector is at 35%, if any individual stock within tech is >25% of the portfolio. Check each holding: value of the largest position / total portfolio value should be ≤ 25%. If any single stock is above 25%, you’d need to trim or diversify that position.\n",
      "- Stay vigilant about drift: Market movements can push sector exposure above 35% over time. If you expect tech to rise or if you plan new purchases, rebalance to keep tech at or below 35%.\n",
      "\n",
      "Recommended steps to verify:\n",
      "1) Compute tech allocation: sum of all tech holdings / total portfolio value = 35% (currently at the limit).\n",
      "2) For each equity, compute position size: single position value / total portfolio value ≤ 25%.\n",
      "3) If needed, rebalance by trimming the largest tech holdings or increasing non-tech exposure to maintain the limits.\n",
      "\n",
      "If you want, share your current holdings and totals and I can run a quick check for you.\n",
      "\n",
      "💡 Recommendations: 3\n",
      "   1. Short answer: Yes, 35% in technology stocks is compliant with the stated sector limit, since the rule says the technology allocation should “not exceed 35%” of the portfolio. Being at 35% exactly meets the limit..\n",
      "   2. - No single equity position: The second rule (no single equity position shall exceed 25% of total portfolio value) could still be violated even if tech as a sector is at 35%, if any individual stock within tech is >25% of the portfolio. Check each holding: value of the largest position / total portfolio value should be ≤ 25%. If any single stock is above 25%, you’d need to trim or diversify that position..\n",
      "   3. Recommended steps to verify:.\n"
     ]
    }
   ],
   "source": [
    "# Demo 1: Portfolio Concentration Analysis\n",
    "print(\"🎪 Demo 1: Portfolio Concentration Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "demo1_query = \"I have 35% of my portfolio in technology stocks. Is this compliant with concentration limits?\"\n",
    "demo1_user = \"portfolio_manager_alice\"\n",
    "\n",
    "result1 = run_agentic_analysis(demo1_query, demo1_user)\n",
    "display_results(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎪 Demo 2: ESG Compliance Check\n",
      "============================================================\n",
      "🚀 Starting agentic analysis for user: portfolio_manager_alice\n",
      "Query: I'm looking at a stock with an ESG score of 5.5. Should I include it in our sustainable portfolio?\n",
      "Session: session_ece4fd79\n",
      "\n",
      "📊 Using LangGraph workflow...\n",
      "\n",
      "🔄 Step 1: Memory Recall Node\n",
      "🧠 Memory Recall: 'I'm looking at a stock with an ESG score of 5.5. Should I include it in our sustainable portfolio?' for user portfolio_manager_alice\n",
      "   Found 3 relevant memories\n",
      "   ✅ Recalled 3 memories\n",
      "\n",
      "🔄 Step 2: RAG Retrieval Node\n",
      "🔍 RAG Retrieval: 'I'm looking at a stock with an ESG score of 5.5. Should I include it in our sustainable portfolio?' (categories: None)\n",
      "\n",
      "🔍 Enhanced RAG search for: 'I'm looking at a stock with an ESG score of 5.5. Should I include it in our sustainable portfolio?'\n",
      "✅ Found 5 results\n",
      "   1. Score: 0.703 | Category: esg_compliance | Severity: high\n",
      "      Text: ESG score below 6.0 requires additional review and justification for inclusion in sustainable portfo...\n",
      "      Regulation: eu_taxonomy\n",
      "   2. Score: 0.421 | Category: concentration_limits | Severity: high\n",
      "      Text: No single equity position shall exceed 25% of total portfolio value to maintain diversification and ...\n",
      "      Regulation: basel_iii\n",
      "   3. Score: 0.407 | Category: sector_limits | Severity: medium\n",
      "      Text: Technology sector allocation should not exceed 35% of total portfolio to avoid overexposure to secto...\n",
      "      Regulation: internal_policy\n",
      "   4. Score: 0.382 | Category: alternative_assets | Severity: high\n",
      "      Text: Cryptocurrency exposure must not exceed 5% of total portfolio value due to regulatory uncertainty an...\n",
      "      Regulation: sec_guidance\n",
      "   5. Score: 0.342 | Category: liquidity | Severity: medium\n",
      "      Text: Cash allocation should remain between 5-15% for liquidity management and opportunistic investments d...\n",
      "      Regulation: internal_policy\n",
      "   ✅ Retrieved 5 compliance rules\n",
      "\n",
      "🔄 Step 3: Analysis Node\n",
      "🤖 LLM Reasoning: 1038 chars\n",
      "   Chat completion truncated/empty (finish_reason=length); retrying with shorter prompt\n",
      "   ✅ Generated analysis (178 chars)\n",
      "   ✅ Extracted 0 recommendations\n",
      "\n",
      "🔄 Step 4: Memorization Node\n",
      "💾 Memory Save: Saving item for user portfolio_manager_alice\n",
      "   Save successful\n",
      "   ✅ Memory save successful\n",
      "   🔄 Converting dict result to AgentState...\n",
      "\n",
      "🎯 Analysis Results for Session: session_ece4fd79\n",
      "User: portfolio_manager_alice\n",
      "Steps completed: 4\n",
      "\n",
      "📊 Compliance Rules Retrieved: 5\n",
      "   1. esg_compliance: ESG score below 6.0 requires additional review and justification for inclusion i... (Score: 0.703)\n",
      "   2. concentration_limits: No single equity position shall exceed 25% of total portfolio value to maintain ... (Score: 0.421)\n",
      "   3. sector_limits: Technology sector allocation should not exceed 35% of total portfolio to avoid o... (Score: 0.407)\n",
      "\n",
      "🧠 Memories Recalled: 3\n",
      "   1. ...\n",
      "   2. ...\n",
      "\n",
      "📝 Analysis Result:\n",
      "   Based on the retrieved compliance rules, a detailed analysis is required to determine compliance status. Please review the specific rules and thresholds mentioned in the context.\n",
      "\n",
      "💡 Recommendations: 0\n"
     ]
    }
   ],
   "source": [
    "# Demo 2: ESG Compliance Check (same user to test memory)\n",
    "print(\"\\n🎪 Demo 2: ESG Compliance Check\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "demo2_query = \"I'm looking at a stock with an ESG score of 5.5. Should I include it in our sustainable portfolio?\"\n",
    "# Same user to test memory recall\n",
    "\n",
    "result2 = run_agentic_analysis(demo2_query, demo1_user)  # Same user\n",
    "display_results(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎪 Demo 3: Complex Multi-Category Analysis\n",
      "============================================================\n",
      "🚀 Starting agentic analysis for user: risk_analyst_bob\n",
      "Query: I want to add 8% cryptocurrency exposure and maintain 12% cash. What are the compliance implications?\n",
      "Session: session_7d125e80\n",
      "\n",
      "📊 Using LangGraph workflow...\n",
      "\n",
      "🔄 Step 1: Memory Recall Node\n",
      "🧠 Memory Recall: 'I want to add 8% cryptocurrency exposure and maintain 12% cash. What are the compliance implications?' for user risk_analyst_bob\n",
      "   Found 3 relevant memories\n",
      "   ✅ Recalled 3 memories\n",
      "\n",
      "🔄 Step 2: RAG Retrieval Node\n",
      "🔍 RAG Retrieval: 'I want to add 8% cryptocurrency exposure and maintain 12% cash. What are the compliance implications?' (categories: None)\n",
      "\n",
      "🔍 Enhanced RAG search for: 'I want to add 8% cryptocurrency exposure and maintain 12% cash. What are the compliance implications?'\n",
      "✅ Found 5 results\n",
      "   1. Score: 0.633 | Category: alternative_assets | Severity: high\n",
      "      Text: Cryptocurrency exposure must not exceed 5% of total portfolio value due to regulatory uncertainty an...\n",
      "      Regulation: sec_guidance\n",
      "   2. Score: 0.467 | Category: concentration_limits | Severity: high\n",
      "      Text: No single equity position shall exceed 25% of total portfolio value to maintain diversification and ...\n",
      "      Regulation: basel_iii\n",
      "   3. Score: 0.466 | Category: liquidity | Severity: medium\n",
      "      Text: Cash allocation should remain between 5-15% for liquidity management and opportunistic investments d...\n",
      "      Regulation: internal_policy\n",
      "   4. Score: 0.410 | Category: sector_limits | Severity: medium\n",
      "      Text: Technology sector allocation should not exceed 35% of total portfolio to avoid overexposure to secto...\n",
      "      Regulation: internal_policy\n",
      "   5. Score: 0.370 | Category: esg_compliance | Severity: high\n",
      "      Text: ESG score below 6.0 requires additional review and justification for inclusion in sustainable portfo...\n",
      "      Regulation: eu_taxonomy\n",
      "   ✅ Retrieved 5 compliance rules\n",
      "\n",
      "🔄 Step 3: Analysis Node\n",
      "🤖 LLM Reasoning: 1011 chars\n",
      "   Chat completion truncated/empty (finish_reason=length); retrying with shorter prompt\n",
      "   ✅ Generated analysis (178 chars)\n",
      "   ✅ Extracted 0 recommendations\n",
      "\n",
      "🔄 Step 4: Memorization Node\n",
      "💾 Memory Save: Saving item for user risk_analyst_bob\n",
      "   Save successful\n",
      "   ✅ Memory save successful\n",
      "   🔄 Converting dict result to AgentState...\n",
      "\n",
      "🎯 Analysis Results for Session: session_7d125e80\n",
      "User: risk_analyst_bob\n",
      "Steps completed: 4\n",
      "\n",
      "📊 Compliance Rules Retrieved: 5\n",
      "   1. alternative_assets: Cryptocurrency exposure must not exceed 5% of total portfolio value due to regul... (Score: 0.633)\n",
      "   2. concentration_limits: No single equity position shall exceed 25% of total portfolio value to maintain ... (Score: 0.467)\n",
      "   3. liquidity: Cash allocation should remain between 5-15% for liquidity management and opportu... (Score: 0.466)\n",
      "\n",
      "🧠 Memories Recalled: 3\n",
      "   1. ...\n",
      "   2. ...\n",
      "\n",
      "📝 Analysis Result:\n",
      "   Based on the retrieved compliance rules, a detailed analysis is required to determine compliance status. Please review the specific rules and thresholds mentioned in the context.\n",
      "\n",
      "💡 Recommendations: 0\n"
     ]
    }
   ],
   "source": [
    "# Demo 3: Complex Multi-Category Query\n",
    "print(\"\\n🎪 Demo 3: Complex Multi-Category Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "demo3_query = \"I want to add 8% cryptocurrency exposure and maintain 12% cash. What are the compliance implications?\"\n",
    "demo3_user = \"risk_analyst_bob\"\n",
    "\n",
    "result3 = run_agentic_analysis(demo3_query, demo3_user)\n",
    "display_results(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing Fixed Compliance Analysis System\n",
      "============================================================\n",
      "Query: I have 35% of my portfolio in technology stocks. Is this compliant with concentration limits?\n",
      "\n",
      "📊 Testing Deterministic Analysis:\n",
      "----------------------------------------\n",
      "\n",
      "🔍 Enhanced RAG search for: 'I have 35% of my portfolio in technology stocks. Is this compliant with concentration limits?'\n",
      "✅ Found 5 results\n",
      "   1. Score: 0.647 | Category: sector_limits | Severity: medium\n",
      "      Text: Technology sector allocation should not exceed 35% of total portfolio to avoid overexposure to secto...\n",
      "      Regulation: internal_policy\n",
      "   2. Score: 0.606 | Category: concentration_limits | Severity: high\n",
      "      Text: No single equity position shall exceed 25% of total portfolio value to maintain diversification and ...\n",
      "      Regulation: basel_iii\n",
      "   3. Score: 0.550 | Category: alternative_assets | Severity: high\n",
      "      Text: Cryptocurrency exposure must not exceed 5% of total portfolio value due to regulatory uncertainty an...\n",
      "      Regulation: sec_guidance\n",
      "   4. Score: 0.409 | Category: liquidity | Severity: medium\n",
      "      Text: Cash allocation should remain between 5-15% for liquidity management and opportunistic investments d...\n",
      "      Regulation: internal_policy\n",
      "   5. Score: 0.401 | Category: esg_compliance | Severity: high\n",
      "      Text: ESG score below 6.0 requires additional review and justification for inclusion in sustainable portfo...\n",
      "      Regulation: eu_taxonomy\n",
      "==================================================\n",
      "📊 COMPLIANCE SUMMARY\n",
      "==================================================\n",
      "\n",
      "🎯 Portfolio Allocations:\n",
      "   • Technology Sector: 35.0%\n",
      "\n",
      "⚠️ WARNINGS (1):\n",
      "   ⚠️ AT LIMIT: Technology Sector Limit - 35.0% exactly at 35.0% limit\n",
      "\n",
      "==================================================\n",
      "🟡 OVERALL STATUS: COMPLIANT WITH WARNINGS\n",
      "💡 Recommendation: Monitor closely and consider rebalancing\n",
      "==================================================\n",
      "\n",
      "============================================================\n",
      "🚀 Testing Full Agentic System with Fixes:\n",
      "----------------------------------------\n",
      "🚀 Starting agentic analysis for user: test_user_fixed\n",
      "Query: I have 35% of my portfolio in technology stocks. Is this compliant with concentration limits?\n",
      "Session: session_a54aeef9\n",
      "\n",
      "📊 Using LangGraph workflow...\n",
      "\n",
      "🔄 Step 1: Memory Recall Node\n",
      "🧠 Memory Recall: 'I have 35% of my portfolio in technology stocks. Is this compliant with concentration limits?' for user test_user_fixed\n",
      "   Found 0 relevant memories\n",
      "   ✅ Recalled 0 memories\n",
      "\n",
      "🔄 Step 2: RAG Retrieval Node\n",
      "🔍 RAG Retrieval: 'I have 35% of my portfolio in technology stocks. Is this compliant with concentration limits?' (categories: None)\n",
      "\n",
      "🔍 Enhanced RAG search for: 'I have 35% of my portfolio in technology stocks. Is this compliant with concentration limits?'\n",
      "✅ Found 5 results\n",
      "   1. Score: 0.647 | Category: sector_limits | Severity: medium\n",
      "      Text: Technology sector allocation should not exceed 35% of total portfolio to avoid overexposure to secto...\n",
      "      Regulation: internal_policy\n",
      "   2. Score: 0.606 | Category: concentration_limits | Severity: high\n",
      "      Text: No single equity position shall exceed 25% of total portfolio value to maintain diversification and ...\n",
      "      Regulation: basel_iii\n",
      "   3. Score: 0.550 | Category: alternative_assets | Severity: high\n",
      "      Text: Cryptocurrency exposure must not exceed 5% of total portfolio value due to regulatory uncertainty an...\n",
      "      Regulation: sec_guidance\n",
      "   4. Score: 0.409 | Category: liquidity | Severity: medium\n",
      "      Text: Cash allocation should remain between 5-15% for liquidity management and opportunistic investments d...\n",
      "      Regulation: internal_policy\n",
      "   5. Score: 0.401 | Category: esg_compliance | Severity: high\n",
      "      Text: ESG score below 6.0 requires additional review and justification for inclusion in sustainable portfo...\n",
      "      Regulation: eu_taxonomy\n",
      "   ✅ Retrieved 5 compliance rules\n",
      "\n",
      "🔄 Step 3: Analysis Node\n",
      "🤖 LLM Reasoning: 988 chars\n",
      "   Chat completion truncated/empty (finish_reason=length); retrying with shorter prompt\n",
      "   Generated 1526 chars\n",
      "   ✅ Generated analysis (1526 chars)\n",
      "   ✅ Extracted 2 recommendations\n",
      "\n",
      "🔄 Step 4: Memorization Node\n",
      "💾 Memory Save: Saving item for user test_user_fixed\n",
      "   Save successful\n",
      "   ✅ Memory save successful\n",
      "   🔄 Converting dict result to AgentState...\n",
      "\n",
      "🎯 Analysis Results for Session: session_a54aeef9\n",
      "User: test_user_fixed\n",
      "Steps completed: 4\n",
      "\n",
      "📊 Compliance Rules Retrieved: 5\n",
      "   1. sector_limits: Technology sector allocation should not exceed 35% of total portfolio to avoid o... (Score: 0.647)\n",
      "   2. concentration_limits: No single equity position shall exceed 25% of total portfolio value to maintain ... (Score: 0.606)\n",
      "   3. alternative_assets: Cryptocurrency exposure must not exceed 5% of total portfolio value due to regul... (Score: 0.550)\n",
      "\n",
      "🧠 Memories Recalled: 0\n",
      "\n",
      "📝 Analysis Result:\n",
      "   Short answer: Yes, 35% tech exposure is compliant with the stated sector limit, as long as the limit is inclusive (not exceeding 35%).\n",
      "\n",
      "Key caveat:\n",
      "- The sector rule caps technology at 35% of the portfolio. If you truly have exactly 35%, you’re at the limit but compliant.\n",
      "- The other rule says no single equity position may exceed 25% of the portfolio. This is a separate constraint. If any individual stock within your tech holdings (or elsewhere) is greater than 25% of the total portfolio value, you’d be in violation—even if your total tech allocation is at or below 35%.\n",
      "\n",
      "What to verify/do:\n",
      "- Check each individual holding: ensure no single stock > 25% of the total portfolio value.\n",
      "- If you have a stock that’s e.g., 28–30% of the portfolio, you’ll need to trim or diversify that position to bring it under 25%.\n",
      "- If your tech slice is 35% but composed of many smaller tech positions, you still need to check each one for the 25% cap.\n",
      "\n",
      "Recommended actions to stay compliant:\n",
      "- If you’re at or near the 35% tech cap, set up a follow-up check or alert to ensure you don’t drift above 35%.\n",
      "- Consider rebalancing to create a cushion below the 25% cap for the largest position (e.g., keep all individual positions at or below 25% and keep tech exposure within a target range like 30–35%).\n",
      "- Maintain a monitoring process (monthly or quarterly) to verify both sector and single-position limits.\n",
      "\n",
      "If you’d like, share your current holdings or the exact portfolio values, and I can run through a concrete check for both rules.\n",
      "\n",
      "💡 Recommendations: 2\n",
      "   1. Recommended actions to stay compliant:.\n",
      "   2. - Consider rebalancing to create a cushion below the 25% cap for the largest position (e.g., keep all individual positions at or below 25% and keep tech exposure within a target range like 30–35%)..\n",
      "\n",
      "⚠️ PARTIAL: Analysis generated but may need refinement\n",
      "\n",
      "🎯 Key Improvements Implemented:\n",
      "   ✅ Fixed Letta memory recall parameter handling\n",
      "   ✅ Fixed OpenAI API response object handling\n",
      "   ✅ Added deterministic compliance checking\n",
      "   ✅ Fallback analysis when LLM fails\n",
      "   ✅ Structured constraint parsing\n",
      "   ✅ Clear compliance status determination\n"
     ]
    }
   ],
   "source": [
    "# Test the Fixed System with Deterministic Analysis\n",
    "print(\"🧪 Testing Fixed Compliance Analysis System\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test with the original portfolio concentration query\n",
    "test_query = \"I have 35% of my portfolio in technology stocks. Is this compliant with concentration limits?\"\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print()\n",
    "\n",
    "# Test deterministic analysis directly first\n",
    "print(\"📊 Testing Deterministic Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Get the rules that would be retrieved\n",
    "test_rules = enhanced_rag_search_fixed(test_query, top_k=5)\n",
    "deterministic_result = analyze_portfolio_compliance(test_query, test_rules)\n",
    "print(deterministic_result)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🚀 Testing Full Agentic System with Fixes:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Test the full system with fixes\n",
    "result = run_agentic_analysis(test_query, \"test_user_fixed\")\n",
    "display_results(result)\n",
    "\n",
    "# Verify we get a proper analysis\n",
    "if result.analysis_result and \"COMPLIANCE SUMMARY\" in result.analysis_result:\n",
    "    print(\"\\n✅ SUCCESS: System now provides deterministic compliance analysis!\")\n",
    "    print(\"✅ The system correctly identifies that 35% tech exposure is compliant but at the limit\")\n",
    "else:\n",
    "    print(\"\\n⚠️ PARTIAL: Analysis generated but may need refinement\")\n",
    "\n",
    "print(\"\\n🎯 Key Improvements Implemented:\")\n",
    "print(\"   ✅ Fixed Letta memory recall parameter handling\")\n",
    "print(\"   ✅ Fixed OpenAI API response object handling\") \n",
    "print(\"   ✅ Added deterministic compliance checking\")\n",
    "print(\"   ✅ Fallback analysis when LLM fails\")\n",
    "print(\"   ✅ Structured constraint parsing\")\n",
    "print(\"   ✅ Clear compliance status determination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Final System Validation\n",
      "============================================================\n",
      "🔍 Component Status:\n",
      "   ✅ rag_system: OK\n",
      "   ✅ memory_service: OK\n",
      "   ✅ langgraph_workflow: OK\n",
      "   ✅ telemetry: OK\n",
      "   ✅ demo1_success: OK\n",
      "   ✅ demo2_success: OK\n",
      "   ✅ demo3_success: OK\n",
      "\n",
      "🎯 Overall System Health: 7/7 components operational\n",
      "\n",
      "🚀 Agentic System Successfully Demonstrated!\n",
      "\n",
      "Key Features Showcased:\n",
      "   ✅ RAG-based compliance rule retrieval\n",
      "   ✅ Persistent agent memory with Letta\n",
      "   ✅ Multi-step LangGraph workflows\n",
      "   ✅ OpenTelemetry + LangSmith tracing\n",
      "   ✅ Pydantic v2 state management\n",
      "   ✅ End-to-end agentic reasoning\n",
      "\n",
      "💡 Next Steps:\n",
      "   • Integrate with FastAPI backend\n",
      "   • Add streaming responses\n",
      "   • Enhance memory categorization\n",
      "   • Add multi-agent collaboration\n",
      "   • Implement HITL workflows\n",
      "\n",
      "📊 Demo Statistics:\n",
      "   • Users created: 2\n",
      "   • Sessions executed: 3\n",
      "   • Total compliance rules: 6\n",
      "   • Qdrant collection: nb2_fixed_portfolio_rules\n",
      "   • Memory service: Letta\n",
      "\n",
      "============================================================\n",
      "🎉 RAG + Letta Agentic System Demo Complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Final System Validation and Summary\n",
    "print(\"\\n🧪 Final System Validation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "validation_results = {\n",
    "    \"rag_system\": len(enhanced_compliance_rules) > 0,\n",
    "    \"memory_service\": BACKEND_AVAILABLE and memory_health().get('letta_available', False),\n",
    "    \"langgraph_workflow\": LANGGRAPH_AVAILABLE and agent_graph is not None,\n",
    "    \"telemetry\": tracer is not None,\n",
    "    \"demo1_success\": not hasattr(result1, 'error_message') or result1.error_message is None,\n",
    "    \"demo2_success\": not hasattr(result2, 'error_message') or result2.error_message is None,\n",
    "    \"demo3_success\": not hasattr(result3, 'error_message') or result3.error_message is None,\n",
    "}\n",
    "\n",
    "print(\"🔍 Component Status:\")\n",
    "for component, status in validation_results.items():\n",
    "    status_icon = \"✅\" if status else \"❌\"\n",
    "    print(f\"   {status_icon} {component}: {'OK' if status else 'FAILED'}\")\n",
    "\n",
    "total_passed = sum(validation_results.values())\n",
    "total_components = len(validation_results)\n",
    "\n",
    "print(f\"\\n🎯 Overall System Health: {total_passed}/{total_components} components operational\")\n",
    "\n",
    "if total_passed >= 5:  # Most components working\n",
    "    print(\"\\n🚀 Agentic System Successfully Demonstrated!\")\n",
    "    print(\"\\nKey Features Showcased:\")\n",
    "    print(\"   ✅ RAG-based compliance rule retrieval\")\n",
    "    print(\"   ✅ Persistent agent memory with Letta\") \n",
    "    print(\"   ✅ Multi-step LangGraph workflows\")\n",
    "    print(\"   ✅ OpenTelemetry + LangSmith tracing\")\n",
    "    print(\"   ✅ Pydantic v2 state management\")\n",
    "    print(\"   ✅ End-to-end agentic reasoning\")\n",
    "    \n",
    "    print(\"\\n💡 Next Steps:\")\n",
    "    print(\"   • Integrate with FastAPI backend\")\n",
    "    print(\"   • Add streaming responses\")\n",
    "    print(\"   • Enhance memory categorization\") \n",
    "    print(\"   • Add multi-agent collaboration\")\n",
    "    print(\"   • Implement HITL workflows\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n⚠️ System partially operational - check component failures above\")\n",
    "\n",
    "print(f\"\\n📊 Demo Statistics:\")\n",
    "print(f\"   • Users created: 2\")\n",
    "print(f\"   • Sessions executed: 3\") \n",
    "print(f\"   • Total compliance rules: {len(enhanced_compliance_rules)}\")\n",
    "print(f\"   • Qdrant collection: {COLLECTION_NAME}\")\n",
    "print(f\"   • Memory service: {'Letta' if BACKEND_AVAILABLE else 'Mock'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎉 RAG + Letta Agentic System Demo Complete!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
